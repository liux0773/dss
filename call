import pandas as pd

import numpy as np

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.decomposition import LatentDirichletAllocation

 

# Sample predefined option contents (you should adjust these)

option_a = "Artificial intelligence is transforming industries."

option_b = "Machine learning is a subset of AI."

option_c = "Deep learning improves neural networks."

option_d = "Football is a popular sport worldwide."

 

# Example survey responses (delimited by ';')

documents = [

    f"{option_a}; {option_c}; Customer feedback on AI trends and regulations; dasdasdas",

    f"{option_b}; {option_d}; Future impact of AI on daily life.",

    f"{option_a}; Customer preference for AI applications in healthcare.",

    f"{option_d}; New sports technologies affecting performance."

]

 

# Function to extract options and "Other" text

def extract_options_and_other(response, options):

    split_text = [text.strip() for text in response.split(';')]

    selected_options = {opt: (opt in split_text) for opt in options}

    other_texts = [text for text in split_text if text not in options]  # Exclude predefined options

    return selected_options, ';'.join(other_texts)

 

# Process survey responses

options = [option_a, option_b, option_c, option_d]

processed_documents = [extract_options_and_other(response, options) for response in documents]

 

# Convert structured data into DataFrame

df = pd.DataFrame([dict(**opts, Other=''.join(other)) for opts, other in processed_documents])

 

# Convert text to count-based representation

vectorizer = CountVectorizer(stop_words='english')

X = vectorizer.fit_transform(df['Other'])

 

# Apply LDA for topic modeling

num_topics = 3  # Adjust based on the expected topic range

lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)

lda_model.fit(X)

 

# Get topic-word distribution

words = vectorizer.get_feature_names_out()

topic_word_distributions = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]

 

# Display top words per topic

for topic_idx, topic in enumerate(topic_word_distributions):

    top_words = [words[i] for i in topic.argsort()[-5:]]

    print(f"Topic {topic_idx}: {', '.join(top_words)}")

 

# Get topic distribution for each document

doc_topic_distributions = lda_model.transform(X)

 

#%%

# Option 1 Assign topics to documents based on how many topics are present

df_option1 = df.copy()

num_of_topics_selected = 2

# Extract top N topics per document

topics_per_doc = [topic.argsort()[-num_of_topics_selected:] for topic in doc_topic_distributions]

# Add individual topic columns

for i in range(num_of_topics_selected):

    df_option1[f'Topic {i + 1}'] = [topics[i] for topics in topics_per_doc]

print(df_option1)

 

#%%

# Option 2 Assign topics based on probability threshold

df_option2 = df.copy()

threshold = 0.25

topics = lda_model.transform(X)

topic_labels = [f"Topic {i+1}" for i in range(topics.shape[1])]

 

for i, topic_name in enumerate(topic_labels):

    df_option2[topic_name] = (topics[:, i] > threshold).astype(int)  # Assign 1 if topic exceeds threshold, otherwise 0

 

# Final structured DataFrame

print(df_option2)
